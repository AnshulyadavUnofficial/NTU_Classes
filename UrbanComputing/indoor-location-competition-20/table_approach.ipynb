{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6120c758",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from io_f import read_data_file\n",
    "from compute_f import split_ts_seq, compute_step_positions\n",
    "import numpy as np\n",
    "from pprint import pprint\n",
    "from typing import Dict, List\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "import os\n",
    "import pandas as pd\n",
    "from visualize_f import visualize_trajectory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ec50ed05",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calibrate_magnetic_wifi_ibeacon_to_position(path_file_list):\n",
    "    mwi_datas = {}\n",
    "    for path_filename in path_file_list:\n",
    "        # print(f'Processing {path_filename}...')\n",
    "\n",
    "        path_datas = read_data_file(path_filename)\n",
    "        acce_datas = path_datas.acce\n",
    "        magn_datas = path_datas.magn\n",
    "        ahrs_datas = path_datas.ahrs\n",
    "        wifi_datas = path_datas.wifi\n",
    "        ibeacon_datas = path_datas.ibeacon\n",
    "        posi_datas = path_datas.waypoint\n",
    "\n",
    "        step_positions = compute_step_positions(acce_datas, ahrs_datas, posi_datas)\n",
    "        # visualize_trajectory(posi_datas[:, 1:3], floor_plan_filename, width_meter, height_meter, title='Ground Truth', show=True)\n",
    "        # visualize_trajectory(step_positions[:, 1:3], floor_plan_filename, width_meter, height_meter, title='Step Position', show=True)\n",
    "        # print(f\"posi_datas[:,1:3]: {posi_datas[:,1:3]}\")\n",
    "        # print(f\"step_positions[:,1:3]: {step_positions[:,1:3]}\")\n",
    "    \n",
    "        if wifi_datas.size != 0:\n",
    "            sep_tss = np.unique(wifi_datas[:, 0].astype(float))\n",
    "            wifi_datas_list = split_ts_seq(wifi_datas, sep_tss)\n",
    "            for wifi_ds in wifi_datas_list:\n",
    "                diff = np.abs(step_positions[:, 0] - float(wifi_ds[0, 0]))\n",
    "                index = np.argmin(diff)\n",
    "                target_xy_key = tuple(step_positions[index, 1:3])   # indexing is exclusive it is step_positions[index,1] and step_positions[index,2]\n",
    "                if target_xy_key in mwi_datas:\n",
    "                    mwi_datas[target_xy_key]['wifi'] = np.append(mwi_datas[target_xy_key]['wifi'], wifi_ds, axis=0)\n",
    "                else:\n",
    "                    mwi_datas[target_xy_key] = {\n",
    "                        'magnetic': np.zeros((0, 4)),\n",
    "                        'wifi': wifi_ds,\n",
    "                        'ibeacon': np.zeros((0, 3))\n",
    "                    }\n",
    "\n",
    "        if ibeacon_datas.size != 0:\n",
    "            sep_tss = np.unique(ibeacon_datas[:, 0].astype(float))\n",
    "            ibeacon_datas_list = split_ts_seq(ibeacon_datas, sep_tss)\n",
    "            for ibeacon_ds in ibeacon_datas_list:\n",
    "                diff = np.abs(step_positions[:, 0] - float(ibeacon_ds[0, 0]))\n",
    "                index = np.argmin(diff)\n",
    "                target_xy_key = tuple(step_positions[index, 1:3])\n",
    "                if target_xy_key in mwi_datas:\n",
    "                    mwi_datas[target_xy_key]['ibeacon'] = np.append(mwi_datas[target_xy_key]['ibeacon'], ibeacon_ds, axis=0)\n",
    "                else:\n",
    "                    mwi_datas[target_xy_key] = {\n",
    "                        'magnetic': np.zeros((0, 4)),\n",
    "                        'wifi': np.zeros((0, 5)),\n",
    "                        'ibeacon': ibeacon_ds\n",
    "                    }\n",
    "\n",
    "        sep_tss = np.unique(magn_datas[:, 0].astype(float))\n",
    "        magn_datas_list = split_ts_seq(magn_datas, sep_tss)\n",
    "        for magn_ds in magn_datas_list:\n",
    "            diff = np.abs(step_positions[:, 0] - float(magn_ds[0, 0]))\n",
    "            index = np.argmin(diff)\n",
    "            target_xy_key = tuple(step_positions[index, 1:3])\n",
    "            if target_xy_key in mwi_datas:\n",
    "                mwi_datas[target_xy_key]['magnetic'] = np.append(mwi_datas[target_xy_key]['magnetic'], magn_ds, axis=0)\n",
    "            else:\n",
    "                mwi_datas[target_xy_key] = {\n",
    "                    'magnetic': magn_ds,\n",
    "                    'wifi': np.zeros((0, 5)),\n",
    "                    'ibeacon': np.zeros((0, 3))\n",
    "                }\n",
    "\n",
    "    return mwi_datas\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f4e0c890",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data for site 1.\n",
      "B1 done. 160 files for B1 loaded.\n",
      "F1 done. 120 files for F1 loaded.\n",
      "F2 done. 123 files for F2 loaded.\n",
      "F3 done. 117 files for F3 loaded.\n",
      "F4 done. 122 files for F4 loaded.\n",
      "Loading data for site 2.\n",
      "B1 done. 70 files for B1 loaded.\n",
      "F1 done. 99 files for F1 loaded.\n",
      "F2 done. 45 files for F2 loaded.\n",
      "F3 done. 40 files for F3 loaded.\n",
      "F4 done. 27 files for F4 loaded.\n",
      "F5 done. 44 files for F5 loaded.\n",
      "F6 done. 67 files for F6 loaded.\n",
      "F7 done. 33 files for F7 loaded.\n",
      "F8 done. 28 files for F8 loaded.\n"
     ]
    }
   ],
   "source": [
    "# Loading all site 1 files\n",
    "site_1_floors = ['B1','F1','F2','F3','F4']\n",
    "site_2_floors = ['B1','F1','F2','F3','F4','F5','F6','F7','F8']\n",
    "\n",
    "site_1_mwi_data: Dict = {floor:None for floor in site_1_floors}      # load mwi data for each floor\n",
    "site_2_mwi_data: Dict = {floor:None for floor in site_2_floors}      # load mwi data for each floor\n",
    "\n",
    "# Load site 1 data\n",
    "print(\"Loading data for site 1.\")\n",
    "for floor in site_1_floors:\n",
    "    floor_data_dir = f'./data/site1/{floor}/path_data_files'\n",
    "    \n",
    "    path_filenames = list(Path(floor_data_dir).resolve().glob(\"*.txt\"))\n",
    "    \n",
    "    # get mwi data for each floor\n",
    "    mwi_data = calibrate_magnetic_wifi_ibeacon_to_position(path_filenames)\n",
    "    site_1_mwi_data[floor] = mwi_data\n",
    "    print(f\"{floor} done. {len(path_filenames)} files for {floor} loaded.\")\n",
    "    \n",
    "    \n",
    "# load Site 2 data\n",
    "print(\"Loading data for site 2.\")\n",
    "for floor in site_2_floors:\n",
    "    floor_data_dir = f'./data/site2/{floor}/path_data_files'\n",
    "    \n",
    "    path_filenames = list(Path(floor_data_dir).resolve().glob(\"*.txt\"))\n",
    "    \n",
    "    # get mwi data for each floor\n",
    "    mwi_data = calibrate_magnetic_wifi_ibeacon_to_position(path_filenames)\n",
    "    site_2_mwi_data[floor] = mwi_data\n",
    "    print(f\"{floor} done. {len(path_filenames)} files for {floor} loaded.\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cefc3218",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting experiment for Site 1...\n",
      "Processing floor B1...\n",
      "  Test size: 0.1, Random state: 0, K: 1 -> Mean Dev: 22.88, Median Dev: 2.86\n",
      "  Test size: 0.1, Random state: 0, K: 5 -> Mean Dev: 50.45, Median Dev: 46.91\n",
      "Processing floor F1...\n",
      "  Test size: 0.1, Random state: 0, K: 1 -> Mean Dev: 19.83, Median Dev: 4.01\n",
      "  Test size: 0.1, Random state: 0, K: 5 -> Mean Dev: 37.53, Median Dev: 34.64\n",
      "Processing floor F2...\n",
      "  Test size: 0.1, Random state: 0, K: 1 -> Mean Dev: 26.20, Median Dev: 7.02\n",
      "  Test size: 0.1, Random state: 0, K: 5 -> Mean Dev: 43.11, Median Dev: 40.42\n",
      "Processing floor F3...\n",
      "  Test size: 0.1, Random state: 0, K: 1 -> Mean Dev: 25.35, Median Dev: 10.20\n",
      "  Test size: 0.1, Random state: 0, K: 5 -> Mean Dev: 42.91, Median Dev: 38.71\n",
      "Processing floor F4...\n",
      "  Test size: 0.1, Random state: 0, K: 1 -> Mean Dev: 23.29, Median Dev: 4.31\n",
      "  Test size: 0.1, Random state: 0, K: 5 -> Mean Dev: 42.31, Median Dev: 39.05\n",
      "Results for Site 1:\n",
      "Floor  Test_Size  K  Mean_Dev_Mean  Median_Dev_Mean\n",
      "   B1        0.1  1        22.8785           2.8554\n",
      "   B1        0.1  5        50.4466          46.9075\n",
      "   F1        0.1  1        19.8347           4.0135\n",
      "   F1        0.1  5        37.5306          34.6448\n",
      "   F2        0.1  1        26.1985           7.0195\n",
      "   F2        0.1  5        43.1089          40.4180\n",
      "   F3        0.1  1        25.3455          10.2034\n",
      "   F3        0.1  5        42.9079          38.7093\n",
      "   F4        0.1  1        23.2876           4.3150\n",
      "   F4        0.1  5        42.3085          39.0501\n",
      "\n",
      "================================================================================\n",
      "\n",
      "Starting experiment for Site 2...\n",
      "Processing floor B1...\n",
      "  Test size: 0.1, Random state: 0, K: 1 -> Mean Dev: 19.59, Median Dev: 2.39\n",
      "  Test size: 0.1, Random state: 0, K: 5 -> Mean Dev: 34.41, Median Dev: 31.20\n",
      "Processing floor F1...\n",
      "  Test size: 0.1, Random state: 0, K: 1 -> Mean Dev: 24.41, Median Dev: 5.13\n",
      "  Test size: 0.1, Random state: 0, K: 5 -> Mean Dev: 39.26, Median Dev: 35.23\n",
      "Processing floor F2...\n",
      "  Test size: 0.1, Random state: 0, K: 1 -> Mean Dev: 19.17, Median Dev: 2.02\n",
      "  Test size: 0.1, Random state: 0, K: 5 -> Mean Dev: 38.84, Median Dev: 33.56\n",
      "Processing floor F3...\n",
      "  Test size: 0.1, Random state: 0, K: 1 -> Mean Dev: 17.98, Median Dev: 1.34\n",
      "  Test size: 0.1, Random state: 0, K: 5 -> Mean Dev: 32.09, Median Dev: 25.18\n",
      "Processing floor F4...\n",
      "  Test size: 0.1, Random state: 0, K: 1 -> Mean Dev: 17.51, Median Dev: 1.30\n",
      "  Test size: 0.1, Random state: 0, K: 5 -> Mean Dev: 43.15, Median Dev: 37.24\n",
      "Processing floor F5...\n",
      "  Test size: 0.1, Random state: 0, K: 1 -> Mean Dev: 14.23, Median Dev: 1.93\n",
      "  Test size: 0.1, Random state: 0, K: 5 -> Mean Dev: 32.94, Median Dev: 27.35\n",
      "Processing floor F6...\n",
      "  Test size: 0.1, Random state: 0, K: 1 -> Mean Dev: 24.86, Median Dev: 2.42\n",
      "  Test size: 0.1, Random state: 0, K: 5 -> Mean Dev: 47.08, Median Dev: 41.82\n",
      "Processing floor F7...\n",
      "  Test size: 0.1, Random state: 0, K: 1 -> Mean Dev: 15.61, Median Dev: 1.49\n",
      "  Test size: 0.1, Random state: 0, K: 5 -> Mean Dev: 37.17, Median Dev: 34.88\n",
      "Processing floor F8...\n",
      "  Test size: 0.1, Random state: 0, K: 1 -> Mean Dev: 17.89, Median Dev: 1.83\n",
      "  Test size: 0.1, Random state: 0, K: 5 -> Mean Dev: 31.66, Median Dev: 28.24\n",
      "Results for Site 2:\n",
      "Floor  Test_Size  K  Mean_Dev_Mean  Median_Dev_Mean\n",
      "   B1        0.1  1        19.5874           2.3865\n",
      "   B1        0.1  5        34.4080          31.2031\n",
      "   F1        0.1  1        24.4071           5.1334\n",
      "   F1        0.1  5        39.2558          35.2286\n",
      "   F2        0.1  1        19.1713           2.0234\n",
      "   F2        0.1  5        38.8378          33.5561\n",
      "   F3        0.1  1        17.9812           1.3408\n",
      "   F3        0.1  5        32.0913          25.1813\n",
      "   F4        0.1  1        17.5074           1.2992\n",
      "   F4        0.1  5        43.1505          37.2361\n",
      "   F5        0.1  1        14.2344           1.9253\n",
      "   F5        0.1  5        32.9384          27.3508\n",
      "   F6        0.1  1        24.8596           2.4200\n",
      "   F6        0.1  5        47.0827          41.8211\n",
      "   F7        0.1  1        15.6083           1.4930\n",
      "   F7        0.1  5        37.1686          34.8831\n",
      "   F8        0.1  1        17.8906           1.8335\n",
      "   F8        0.1  5        31.6590          28.2389\n",
      "\n",
      "================================================================================\n",
      "\n",
      "Experiments completed!\n"
     ]
    }
   ],
   "source": [
    "def magnetic_similarity_matrix(X_test, X_train, k=5):\n",
    "    \"\"\"\n",
    "    Optimized magnetic similarity matrix with top-k mean.\n",
    "    \"\"\"\n",
    "    T = X_test.shape[0]\n",
    "    J = X_train.shape[0]\n",
    "    similarities = np.zeros((T, J), dtype=np.float32)\n",
    "    \n",
    "    # Precompute masks for test sequences\n",
    "    mask_test = ~np.isnan(X_test[..., 0])\n",
    "    \n",
    "    for i in range(T):\n",
    "        # Get test sequence i\n",
    "        Xi = X_test[i]\n",
    "        mask_i = mask_test[i]\n",
    "        valid_i = Xi[mask_i]\n",
    "    \n",
    "        # Normalize valid test vectors\n",
    "        norm_i = np.linalg.norm(valid_i, axis=1, keepdims=True)\n",
    "        valid_i_norm = valid_i / np.where(norm_i > 0, norm_i, 1.0)\n",
    "        \n",
    "        for j in range(J):\n",
    "            # Get training sequence j\n",
    "            Xj = X_train[j]\n",
    "            mask_j = ~np.isnan(Xj[:, 0])\n",
    "            valid_j = Xj[mask_j]\n",
    "                \n",
    "            # Normalize valid training vectors\n",
    "            norm_j = np.linalg.norm(valid_j, axis=1, keepdims=True)\n",
    "            valid_j_norm = valid_j / np.where(norm_j > 0, norm_j, 1.0)\n",
    "            \n",
    "            # Compute pairwise cosine similarities\n",
    "            S = valid_i_norm @ valid_j_norm.T\n",
    "            \n",
    "            # Apply top-k mean\n",
    "            flat_S = S.ravel()\n",
    "            k_val = min(k, flat_S.size)\n",
    "            # Get top-k values using partition for efficiency\n",
    "            topk = np.partition(flat_S, -k_val)[-k_val:]\n",
    "            similarities[i, j] = np.mean(topk)\n",
    "    \n",
    "    return similarities\n",
    "def top_k_pred(similarities, Y_train, K=5):\n",
    "    \"\"\"\n",
    "    Compute predictions using the weighted average of top-K similar training points.\n",
    "    \n",
    "    Args:\n",
    "        similarities: np.ndarray of shape (n_test, n_train), higher = more similar\n",
    "        Y_train: np.ndarray of shape (n_train, 2), coordinates of training points\n",
    "        K: int, number of top similarities to consider\n",
    "    \n",
    "    Returns:\n",
    "        Y_pred: np.ndarray of shape (n_test, 2)\n",
    "    \"\"\"\n",
    "    n_test = similarities.shape[0]\n",
    "    Y_pred = np.zeros((n_test, 2), dtype=float)\n",
    "    \n",
    "    # --- 1. Get top-K indices for each test sample ---\n",
    "    top_k_idx = np.argpartition(-similarities, K-1, axis=1)[:, :K]\n",
    "    \n",
    "    # --- 2. Compute the weights for each top-K neighbor ---\n",
    "    # Get the top K similarities for each test sample\n",
    "    top_k_similarities = np.take_along_axis(similarities, top_k_idx, axis=1)  # shape: (n_test, K)\n",
    "    \n",
    "    # Normalize the similarities so they sum to 1 (to treat them as probabilities/weights)\n",
    "    top_k_weights = top_k_similarities / np.sum(top_k_similarities, axis=1, keepdims=True)  # shape: (n_test, K)\n",
    "    \n",
    "    # --- 3. Compute weighted average of coordinates ---\n",
    "    # Use advanced indexing to get all top-K coordinates at once\n",
    "    top_k_coords = Y_train[top_k_idx]  # shape: (n_test, K, 2)\n",
    "    \n",
    "    # Compute weighted average along axis=1 (the K dimension)\n",
    "    Y_pred = np.sum(top_k_coords * top_k_weights[..., np.newaxis], axis=1)  # shape: (n_test, 2)\n",
    "    \n",
    "    return Y_pred\n",
    "\n",
    "def magnetic_floor_lookup(floor_data, test_size, random_state, k):\n",
    "    \"\"\"\n",
    "    Perform magnetic-only table lookup localization for a single floor using top-K median.\n",
    "    \n",
    "    Args:\n",
    "        floor_data: dict\n",
    "            Dictionary mapping (x, y) coordinates to data, where each value has a \"magnetic\" array.\n",
    "        test_size: float\n",
    "            Fraction of data to use as test set.\n",
    "        random_state: int\n",
    "            Random seed for train/test splitting.\n",
    "        k: int\n",
    "            Number of top similar training points to consider for computing the median prediction.\n",
    "    \n",
    "    Returns:\n",
    "        mean_dev: float\n",
    "            Mean deviation (Euclidean) between predicted and true positions.\n",
    "        median_dev: float\n",
    "            Median deviation (Euclidean) between predicted and true positions.\n",
    "    \"\"\"\n",
    "\n",
    "    # --- 1. Collect lengths of magnetic readings for all positions ---\n",
    "    N_list = [v[\"magnetic\"].shape[0] for v in floor_data.values()]\n",
    "    Nmax = max(N_list)\n",
    "    Nmin = min(N_list)\n",
    "    Nmedian = np.median(N_list)\n",
    "    Nmean = np.mean(N_list)\n",
    "\n",
    "    # print(f\"Magnetic readings per position -> \"\n",
    "    #       f\"min: {Nmin}, mean: {Nmean:.2f}, median: {Nmedian}, max: {Nmax}\")\n",
    "\n",
    "    num_samples = len(floor_data)\n",
    "    X = np.full((num_samples, Nmax, 3), np.nan)\n",
    "    Y = np.zeros((num_samples, 2), dtype=float)\n",
    "\n",
    "    # Collect all values for global normalization\n",
    "    all_x, all_y, all_z = [], [], []\n",
    "\n",
    "    for idx, (coord, data) in enumerate(floor_data.items()):\n",
    "        Y[idx, 0], Y[idx, 1] = coord\n",
    "        mag = data[\"magnetic\"][:, 1:4]  # skip timestamp → use x,y,z\n",
    "        N = mag.shape[0]\n",
    "        X[idx, :N, :] = mag\n",
    "        all_x.extend(mag[:, 0])\n",
    "        all_y.extend(mag[:, 1])\n",
    "        all_z.extend(mag[:, 2])\n",
    "\n",
    "    # --- 2. Global normalization ---\n",
    "    mu = np.array([np.mean(all_x), np.mean(all_y), np.mean(all_z)])\n",
    "    sigma = np.array([np.std(all_x), np.std(all_y), np.std(all_z)])\n",
    "    X = (X - mu) / sigma\n",
    "    X = X.astype(np.float32)\n",
    "    \n",
    "    # --- 3. Train/test split ---\n",
    "    X_train, X_test, Y_train, Y_test = train_test_split(\n",
    "        X, Y, test_size=test_size, random_state=random_state\n",
    "    )\n",
    "    \n",
    "    # --- 4. Compute similarity matrix ---\n",
    "    similarities = magnetic_similarity_matrix(X_test, X_train, k)\n",
    "\n",
    "    # --- 5. Predict using nearest neighbor ---\n",
    "    Y_pred = top_k_pred(similarities, Y_train, K=k)\n",
    "\n",
    "    # --- 6. Compute deviations ---\n",
    "    deviations = np.sqrt(np.sum((Y_pred - Y_test) ** 2, axis=1))\n",
    "    mean_dev = np.mean(deviations)\n",
    "    median_dev = np.median(deviations)\n",
    "\n",
    "    return mean_dev, median_dev\n",
    "\n",
    "def run_magnetic_experiment(site_name, site_data, test_sizes, random_states, k_values):\n",
    "    \"\"\"Run experiment for magnetic floor lookup with different k values\"\"\"\n",
    "    floors = list(site_data.keys())\n",
    "    \n",
    "    results = []\n",
    "    \n",
    "    # Iterate through all combinations\n",
    "    for floor in floors:\n",
    "        print(f\"Processing floor {floor}...\")\n",
    "        floor_data = site_data[floor]\n",
    "        \n",
    "        for ts in test_sizes:\n",
    "            for rs in random_states:\n",
    "                for k in k_values:\n",
    "                    mean_dev, median_dev = magnetic_floor_lookup(floor_data, ts, rs, k)\n",
    "                    results.append((floor, ts, rs, k, mean_dev, median_dev))\n",
    "                    print(f\"  Test size: {ts}, Random state: {rs}, K: {k} \"\n",
    "                          f\"-> Mean Dev: {mean_dev:.2f}, Median Dev: {median_dev:.2f}\")\n",
    "    \n",
    "    # Create DataFrame with all results\n",
    "    df = pd.DataFrame(\n",
    "        results,\n",
    "        columns=['Floor', 'Test_Size', 'Random_State', 'K', 'Mean_Dev', 'Median_Dev']\n",
    "    )\n",
    "    \n",
    "    # Compute averages for each K, averaging over random_states only\n",
    "    avg_results = df.groupby(['Floor', 'Test_Size', 'K']).agg({\n",
    "        'Mean_Dev': 'mean',    # average mean deviation over random states\n",
    "        'Median_Dev': 'mean'   # average median deviation over random states\n",
    "    }).reset_index()\n",
    "\n",
    "    # Round the columns to 4 decimals for clean CSV\n",
    "    avg_results = avg_results.round(4)\n",
    "    \n",
    "    # Flatten column names properly\n",
    "    avg_results.columns = ['Floor', 'Test_Size', 'K', 'Mean_Dev_Mean','Median_Dev_Mean']\n",
    "    \n",
    "    print(f\"Results for {site_name}:\")\n",
    "    print(avg_results.to_string(index=False))\n",
    "    print(\"\\n\" + \"=\"*80 + \"\\n\")\n",
    "    \n",
    "    return df, avg_results\n",
    "\n",
    "\n",
    "# Main execution block\n",
    "if __name__ == \"__main__\":\n",
    "    # Experiment parameters\n",
    "    test_sizes = [0.1]\n",
    "    random_states = [0]\n",
    "    k_values = [1, 5]  # Different k values to test\n",
    "    \n",
    "    # Assuming you have site_1_mwi_data and site_2_mwi_data\n",
    "    # Run experiment for site 1\n",
    "    print(\"Starting experiment for Site 1...\")\n",
    "    df1, pivot1 = run_magnetic_experiment(\n",
    "        \"Site 1\", \n",
    "        site_1_mwi_data, \n",
    "        test_sizes, \n",
    "        random_states, \n",
    "        k_values\n",
    "    )\n",
    "    \n",
    "    # Run experiment for site 2\n",
    "    print(\"Starting experiment for Site 2...\")\n",
    "    df2, pivot2 = run_magnetic_experiment(\n",
    "        \"Site 2\", \n",
    "        site_2_mwi_data, \n",
    "        test_sizes, \n",
    "        random_states, \n",
    "        k_values\n",
    "    )\n",
    "    \n",
    "        # Round all numeric columns to 4 decimal places\n",
    "    df1 = df1.round(4)\n",
    "    df2 = df2.round(4)\n",
    "    pivot1 = pivot1.round(4)\n",
    "    pivot2 = pivot2.round(4)\n",
    "\n",
    "    # Save results to CSV files\n",
    "    df1.to_csv(\"site1_magnetic_topk_results.csv\", index=False)\n",
    "    df2.to_csv(\"site2_magnetic_topk_results.csv\", index=False)\n",
    "    pivot1.to_csv(\"site1_magnetic_topk_summary.csv\", index=False)\n",
    "    pivot2.to_csv(\"site2_magnetic_topk_summary.csv\", index=False)\n",
    "\n",
    "    print(\"Experiments completed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "400bffc1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
